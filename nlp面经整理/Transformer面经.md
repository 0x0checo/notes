**1.Transformer为何使用多头注意力机制？**

a.多头保证了tansformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。
（多头注意力机制将输入的特征向量分成多个子空间，每个注意力头独立地在这些子空间上计算注意力。这样，每个头可以专注于不同的语义或模式（例如，句法结构、语义关系、局部依赖等）。）

b.适合并行计算，因为每个头的计算是独立的，能显著提高计算效率。

**2.layernorm和batchnorm的区别？为什么Transformer使用layernorm？**

bn:在batch维度上（即对一个 mini-batch 中同一维度的特征做归一化）

ln:在特征维度上（即对单个样本的所有特征做归一化）

why: 

✅ 原因 1：Batch 大小通常较小
NLP 或序列任务中，batch size 可能很小（甚至为 1），BN 在这种情况下统计量不稳定，性能下降。
LN 完全不依赖 batch size，保证一致性。

✅ 原因 2：Transformer 的输入是序列
在 Transformer 中，每个 token 都有一个 embedding 向量，LN 是在 embedding 的维度上做归一化，非常自然。
如果用 BN，会在 batch 内不同句子的 token 之间混合统计量，反而破坏语义。
（想想limu的立方体！！！）

✅ 原因 3：训练/推理一致性
BN 在训练和推理阶段使用的统计量不同（训练时用 mini-batch 统计，推理时用移动平均），可能带来不一致问题。
LN 在训练和推理时行为完全一样，更稳定。

Transformer 使用 LayerNorm，因为它对 batch size 不敏感，更适合序列建模，不会破坏 token 的独立性，而且训练和推理阶段行为一致。

**3.残差连接是什么？**

让输入直接跳过若干层网络，加到输出上。（解决梯度消失/爆炸问题）

**4.什么是掩码注意力？**

在生成任务中，*解码器*在预测第t个词的时候，不应该看到未来（t+1。。）的词，所以要对第t个词后面的位置打上掩码，让模型只看当前和之前的token。

**5.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？**

a.增强表达能力：通过为Q和K使用不同的权重矩阵（$W_Q$ 和 $W_K$），Transformer可以将输入向量（如词嵌入）投影到不同的子空间中。

b.解耦查询和键的角色：在注意力机制中，Q表示当前需要关注的“查询”信息，而K表示用于匹配的“键”信息。它们的角色在语义上是不同的：Q决定了“找什么”，K决定了“被找的内容”。使用不同的权重矩阵允许模型为这两种角色学习专门的表示，从而更精确地建模输入之间的关系。

**6.什么是 Positional Encoding（位置编码）？**

给输入的词向量加上位置信息，让transformer能理解序列的顺序问题。

缺点是，它是固定函数，不能根据任务进行调整；只能表示绝对位置；长序列泛化有限。

**7.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？**

矩阵乘法可以并行运算，效率高，计算速度快。

**8.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解**

点乘的方差随dk增大而增大 → softmax 饱和 → 梯度消失。缩放除以dk的平方根，使打分的方差约为1，softmax稳定。

**9.为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？**

避免词向量太小，被位置编码淹没。使得词向量的数值尺度和位置编码在同一尺度，从而保证语义信息和位置信息都能被很好利用。

**10.简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？**

优点：逐位置独立，不依赖其它位置，易并行计算；包含线性和非线形计算，可以学习更复杂模式。

缺点：不考虑上下文，参数量大，对激活函数选择敏感。

**11.Encoder端和Decoder端是如何进行交互的？**

Encoder 和 Decoder 的交互 主要发生在 Decoder 的第二个子层：Encoder-Decoder Attention。在这里，Decoder 的 Query 去“询问” Encoder 的上下文表示（Key/Value），从而在生成目标序列时考虑源序列信息。

**12.Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)**

在Transformer架构中，"Decoder"阶段的多头自注意力（通常指Decoder中的多头自注意力）与Encoder的多头自注意力有所不同，主要体现在以下几个方面：

1. 关注方向：
   - Encoder的多头自注意力：在输入序列的所有位置之间进行双向关注（即全局关注），帮助编码器捕捉输入的上下文信息。
   - Decoder的多头自注意力：在生成过程中采用的是“部分因果”关注机制，通常用“masked”自注意力，确保每个位置只能关注到当前位置及之前的位置，防止未来信息泄露，保证自回归生成的正确性。

2. 机制差异：
   - Encoder中的自注意力：没有任何遮盖（mask），每个位置都可以看见整个输入序列。
   - Decoder中的自注意力：包含遮盖（mask），阻止当前位置关注未来位置，以确保序列生成的因果关系。

3. 作用目的：
   - Encoder的自注意力：提取输入序列的全局特征，用于后续的编码表示。
   - Decoder的自注意力：在生成每个词时，限制模型只能看见已生成的部分，确保输出的连续性和因果关系。

总结：
- Encoder的多头自注意力关注完整序列，帮助编码输入信息。
- Decoder的多头自注意力使用掩码（mask），只关注已生成部分，确保生成的正确性。

**13.Transformer的并行化体现在哪个地方？Decoder端可以做并行化吗？**

1. **Self-Attention 机制的并行化**：
   - Self-attention 是 Transformer 的核心组件，计算时每个 token 的注意力分数不依赖于其他 token 的计算结果。这种无序依赖允许所有 token 的注意力计算通过矩阵运算（如点积）并行完成。具体来说，输入序列的 Query、Key 和 Value 向量通过矩阵乘法一次性计算，时间复杂度为 O(n²)，但计算过程可以高度并行化，利用 GPU 或 TPU 的并行计算能力。
   - 与 RNN 不同，RNN 需要按时间步顺序计算，当前时间步的隐藏状态依赖于前一时间步，导致无法并行。Self-attention 消除了这种时间依赖，显著提升计算效率。

2. **多头注意力（Multi-Head Attention）的并行化**：
   - Transformer 采用多头注意力机制，将输入分成多个子空间，每一头独立计算注意力。这种设计允许不同头的计算并行执行，进一步提高效率。每个头的计算是独立的，互不干扰，可以同时在多个计算单元上运行。
   - 多头机制还增加了模型的表达能力，同时保持并行性。

3. **Feed-Forward 神经网络的并行化**：
   - Transformer 的每一层包含一个逐位置（position-wise）的前馈神经网络（FFN），对序列中的每个 token 独立应用相同的线性变换和激活函数。由于每个 token 的 FFN 计算不依赖其他 token，结果可以并行计算。这种逐位置的操作非常适合 GPU 的并行架构。

4. **层间并行与批量处理**：
   - Transformer 的编码器和解码器由多层堆叠组成，每一层内的计算（如 self-attention 和 FFN）是并行的。虽然层与层之间存在数据依赖（上一层的输出作为下一层的输入），但同一层内的所有操作都可以并行化。
   - 在训练和推理过程中，Transformer 支持批量处理（batch processing），多个序列（句子）可以同时输入模型，矩阵运算进一步放大并行化的优势。

5. **并行化的硬件优化**：
   - Transformer 的矩阵运算（如注意力分数计算、线性变换）非常适合现代硬件加速器（如 GPU 和 TPU）的并行计算架构。这些硬件可以同时处理大量矩阵运算，充分发挥 Transformer 的并行优势。
   - 例如，注意力机制中的 scaled dot-product attention 通过高效的矩阵乘法库（如 cuBLAS）实现并行化。

**总结**：
Transformer 的并行化主要体现在 self-attention 机制（所有 token 同时计算）、多头注意力（各头独立并行）、逐位置的前馈网络（独立计算）以及批量处理。这些特性使 Transformer 能够充分利用现代计算硬件，显著提高训练和推理速度，相比 RNN 和 LSTM 等顺序模型更高效。

关于 Decoder端是否可以做并行化：

- 训练时：
Decoder的自注意力部分可以并行化（使用掩码方式），因为每个位置的注意力可以在同步计算中完成（只关注到已生成的部分）。
但在实际实现中，为了保持因果关系，通常会用掩码（mask）限制每个位置只能看到当前及之前位置。结果是自注意力中的掩码操作会引入一定的串行依赖，限制了完全的时间维度上的并行化，但在每个步骤内的计算仍是高度并行的。

- 推理/生成时（解码阶段）：
这是一个序列逐步生成的过程（自回归），每次生成一个词后，才能进行下一步，导致极大程度上串行化，这是Transformer在推理阶段的瓶颈。
 目前的优化方法包括：

- 缓存机制：在每一步保存前一阶段的注意力计算结果，只更新新位置的计算，从而避免重复全部重新计算。
- 并行化批次：可以同时处理多个序列或多个生成任务，实现一定程度的并行。

**14.简单描述一下wordpiece model 和 byte pair encoding。**

**WordPiece Model**:
- **定义**: WordPiece 是一种子词（subword）分割算法，用于将单词拆分为更小的单元（子词或字符片段），常用于 NLP 模型如 BERT。
- **工作原理**: 从训练语料库构建词汇表，初始为字符集，通过迭代合并高频子词对，生成固定大小的子词词汇表。分割时，优先选择最长的子词匹配。
- **特点**: 平衡了词级别和字符级别的表示，减少未知词（OOV）问题，适合处理多语言和稀有词。
- **应用**: BERT、Electra 等模型的预处理。

你理解得基本没错！用通俗的语言来说，**WordPiece** 就像是把单词“切”成更小的块（子词单元），这样计算机能更好地理解和处理各种语言，尤其是那些复杂或不常见的词。让我来详细但简单地解释一下：

### WordPiece 是什么？
WordPiece 是一种把单词拆分成小片段（subword units）的方法，常用在像 BERT 这样的模型里。它的目标是：
- 让词汇表覆盖更多词，同时保持词汇表大小可控。
- 处理没见过的词（比如新词、拼写错误或外语词）。
- 让模型更好地理解词的组成部分。

### 怎么“切”词？
1. **从字符开始**：WordPiece 先把每个词看成单个字符的组合（比如“playing”拆成“p, l, a, y, i, n, g”）。
2. **合并高频片段**：根据训练数据，WordPiece 会统计哪些字符或小片段经常一起出现（比如“pl”或“ing”），然后把它们合并成一个单元（像“play”或“##ing”）。这里的“##”表示这个单元是词的一部分，不是完整的词。
3. **构建词汇表**：通过不断合并高频的片段，生成一个固定大小的词汇表（比如 30,000 个子词单元）。
4. **切分新词**：遇到新词时，WordPiece 会尽量用词汇表里的子词单元去拼。比如“playing”可能被切成“play”和“##ing”，而没见过的“unplaying”可能被切成“un##”, “play”, “##ing”。

### 举个例子
假设词汇表里有“play”和“##ing”两个子词：
- “playing” → 切成 “play” + “##ing”。
- “played” → 切成 “play” + “##ed”。
- “unplayable” → 切成 “un##” + “play” + “##able”。

如果遇到完全陌生的词，比如“xyzabc”，WordPiece 可能会切成单个字符：“x”, “y”, “z”, “a”, “b”, “c”。

### 为什么这么切？
1. **解决生僻词**：像“anticonstitutionally”这样长而稀有的词，可以通过“anti##”, “constitu##”, “##tion”, “##ally”来表示，不需要整个词都在词汇表里。
2. **节省空间**：不用存所有可能的词，只存常用的子词，词汇表更小，模型更高效。
3. **语言通用**：对中文、日文等非英文语言也能处理，因为它可以切到字符级别。

### 和“把词切成很多小单元”有什么关系？
你的理解很对！WordPiece 就是把词拆成更小的“零件”，这些零件可以是完整的词（像“the”）、词缀（像“##ing”）、甚至单个字符。模型通过这些零件去“拼凑”理解任意词，就像乐高积木一样，灵活又高效。

### 总结
WordPiece 像个聪明的“词切割机”，把词切成小块（子词），让模型既能处理常见词，也能搞定没见过的怪词。它的核心是根据语料库的频率，决定怎么切得最合理，既省空间又保留语义。
**Byte Pair Encoding (BPE)**:
- **定义**: BPE 是一种数据压缩算法，应用于 NLP 中进行子词分割，常见于 Transformer 模型如 GPT。
- **工作原理**: 从字符级开始，统计语料中相邻符号对的频率，迭代合并最高频的符号对，生成子词词汇表。分割时，基于词汇表逐次合并字符。
- **特点**: 无需预定义词汇表大小，动态生成子词，适应性强，处理 OOV 效果好。
- **应用**: GPT、RoBERTa、T5 等模型的 tokenization。

**区别**:
- WordPiece 基于最大化似然选择子词，BPE 基于频率合并。
- WordPiece 常用于 Google 的模型，BPE 更广泛应用于开源模型。
- WordPiece 可能生成更短的子词，BPE 倾向于更长的子词。

**15.Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？**

---

## 1. **学习率设定（Learning Rate Schedule）**

Transformer 里最经典的做法是 **学习率预热（warmup）+衰减**，来自原始论文 *Attention is All You Need*：

学习率随训练步数 $t$ 的变化公式：

$$
\text{lr}(t) = d_{\text{model}}^{-0.5} \cdot \min\left(t^{-0.5},\ t \cdot \text{warmup\_steps}^{-1.5}\right)
$$

* $d_{\text{model}}$：embedding 维度（比如 512 或 768）。
* **warmup\_steps**：预热步数（比如 4000 步），在这之前学习率 **线性增加**；之后 **按 $1/\sqrt{t}$ 衰减**。
* 直观理解：先小心地增加学习率，等模型稳定了再逐步减小。

现代实现（如 Hugging Face）里，可以选择：

* **Adam + warmup + cosine decay**（常见）
* **AdamW + one-cycle LR**（更适合大规模训练）

---

## 2. **Dropout 的设定与位置**

在 Transformer 中，Dropout 用来缓解过拟合，主要出现在以下几个地方：

1. **Attention 权重之后**：

   $$
   \text{Dropout}(\text{Softmax}(QK^T / \sqrt{d_k}))
   $$

   防止模型过度依赖某几个 token。

2. **子层输出（Sub-layer output）之后**：
   在 **Add & Norm** 之前加 Dropout：

   $$
   \text{LayerNorm}(x + \text{Dropout}(\text{Sublayer}(x)))
   $$

3. **前馈网络（FFN）中的隐层**：
   在两个全连接层之间加 Dropout。

4. **输入嵌入（word embedding + positional encoding）之后**：
   有的实现会在输入 embedding 上也加 Dropout。

⚙️ 常见的 Dropout rate：**0.1 \~ 0.3**（原始论文用 0.1）。

---

## 3. **Dropout 在测试时的注意事项**

* **训练时**：Dropout 随机屏蔽一部分神经元。
* **测试/推理时**：Dropout 会自动关闭（框架如 PyTorch / TensorFlow 会帮你处理），并按训练时的期望值缩放权重。
* 因此 **测试时不需要手动加 Dropout**，只要记得 `model.eval()`（PyTorch）或 `training=False`（TF/Keras）。

如果忘记关闭，会导致结果 **随机、性能下降**。

---

✅ 总结：

* **学习率**：warmup + 衰减（经典是 $d_{\text{model}}^{-0.5}$ \* min(...))。
* **Dropout**：用在 **Attention 权重、子层输出、FFN 隐层、输入 embedding**。
* **测试时**：Dropout 自动关闭，只要切到 eval 模式即可。

---

**16.bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？**

BERT 的 mask（掩码）机制与 Transformer 在注意力机制中屏蔽分数的技巧（例如解码器的自回归掩码）有不同目标，因此设计上有所差异。以下简要分析为何 BERT 未采用 Transformer 在注意力处的分数屏蔽技巧：

1. **任务目标不同**：
   - **Transformer（解码器）**：在自回归生成任务中，屏蔽未来 token 的注意力分数（causal masking）是为了确保模型只依赖当前及之前的 token，符合生成任务的顺序依赖。
   - **BERT**：采用掩码语言模型（MLM）任务，随机掩码 15% 的输入 token，目标是预测这些被掩码的 token。BERT 的双向上下文建模需要所有 token（包括未掩码的）同时参与注意力计算，因此不需要像解码器那样的单向屏蔽。

2. **BERT 的掩码机制**：
   - BERT 在输入层直接将部分 token 替换为 [MASK]（80%）、随机 token（10%）或保持原样（10%），然后通过 self-attention 建模整个序列的上下文。
   - 这种掩码发生在输入 embedding 阶段，而非注意力分数的屏蔽。注意力机制本身是全连接的，允许所有 token 相互交互，以捕获双向语义。

3. **为何不屏蔽注意力分数**：
   - **双向性需求**：BERT 的核心是双向建模，屏蔽注意力分数（如 Transformer 解码器的三角掩码）会限制模型看到部分 token，破坏双向上下文学习，降低 MLM 任务效果。
   - **计算效率**：在输入层直接掩码 token 比在注意力分数矩阵上动态屏蔽更简单，计算开销更低，且易于并行化。
   - **任务适配性**：MLM 任务通过掩码 token 模拟“填空”预测，输入层的 [MASK] 标记足以提供缺失信息信号，无需在注意力层额外屏蔽。

4. **技术差异**：
   - Transformer 解码器的注意力屏蔽通过在 softmax 前对分数矩阵施加负无穷大掩码实现，强制未来 token 的注意力权重为 0。
   - BERT 的 MLM 通过替换输入 token 实现掩码，注意力机制保持标准 self-attention 计算，允许所有 token 自由交互。

**总结**：
BERT 未采用 Transformer 注意力分数的屏蔽技巧，因为其 MLM 任务需要双向上下文，输入层掩码已足够实现预测目标。屏蔽注意力分数会限制双向性，且增加不必要的复杂性。







