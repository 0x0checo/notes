解决大模型（LLM）的**幻觉（Hallucination）**——即“一本正经地胡说八道”——是目前 AI 应用落地的核心挑战。

首先要明确一个残酷的事实：**目前的 LLM 架构无法 100% 彻底消除幻觉**。这是因为 LLM 的本质是概率预测机，它不仅在“回忆”事实，更是在“生成”最合理的下一个字。

但是，我们可以通过一套组合拳，将幻觉率降低到**生产可用**的水平（比如从 20% 降到 1% 以下）。

这套组合拳可以分为三个层面：**提示工程层（Prompt）**、**数据检索层（RAG）** 和 **流程/模型层（Process/Model）**。

---

### 第一层：数据检索层（RAG）—— 给模型“开卷考试”

这是目前最有效的方法。如果你不给模型任何背景资料，它只能靠“内隐记忆”瞎编（闭卷考试）；如果你给了它参考文档，它就可以基于文档回答（开卷考试）。

#### 1. 严格限制来源 (Grounding)
在 System Prompt 中强制约束模型：
> "你是一个由 [公司名] 建立的 AI 助手。请**仅根据**下面提供的 `<Context>` 内容回答问题。如果 `<Context>` 中没有答案，**请直接回答“不知道”**，严禁编造信息。"

#### 2. 提高检索精度 (Better Retrieval)
这是我们刚才讲 **Hybrid Search** 的意义所在。
*   **垃圾进，垃圾出 (Garbage In, Garbage Out)：** 如果你检索回来的文档跟用户问题无关，模型为了回答问题，就**被迫**开始编造。
*   **优化：** 使用 Hybrid Search + Re-ranking (重排序) 确保传给 LLM 的 Context 是真正相关的。

#### 3. 引用溯源 (Citations)
要求模型在回答时必须标注来源。这不仅能增加可信度，还能让模型“自我克制”。
> "回答时请在句子末尾标注引用来源，例如 [Doc 1]。如果无法引用任何文档，请不要生成该句子。"

---

### 第二层：提示工程层（Prompt）—— 教模型“好好说话”

#### 1. CoT (Chain of Thought / 思维链)
让模型慢下来思考。幻觉往往发生在模型跳过逻辑步骤直接生成结论时。
*   **Prompt:** "请一步步思考（Let's think step by step）。首先提取文档中的关键事实，然后基于事实进行推理，最后给出结论。"

#### 2. 提供“负面样本” (Few-Shot with Negative Examples)
模型很爱面子，不喜欢说“不知道”。你需要在 Prompt 里给它几个示例（Few-Shot），展示什么样的回答是好的，**特别是要展示“当没有答案时，如何优雅地拒绝回答”的例子**。

#### 3. 角色扮演与语气控制
设定一个严谨的角色。
*   **Prompt:** "你是一位严谨的合规审计员..." 比 "你是一个有创意的助手..." 产生的幻觉要少得多。

---

### 第三层：流程与校验层（Advanced RAG）—— “自我反思”

单一的 LLM 调用往往不可靠，现在流行用 **Agentic Workflow（代理工作流）** 来解决幻觉。

#### 1. CoVE (Chain of Verification / 核验链)
这是 Meta 提出的一种技术。不要让模型一次性生成答案，而是拆分成两步：
1.  **Draft:** 模型先生成一个草稿答案。
2.  **Verify:** 模型生成一组“核实问题”来检查草稿中的事实断言。
3.  **Correct:** 根据核实结果，修正草稿中的错误。

#### 2. Self-Consistency (自洽性 / 多数投票)
对于逻辑推理类问题：
让模型针对同一个问题生成 **3 个不同的答案**。
*   如果 3 个答案都一样，置信度高。
*   如果 3 个答案天差地别，说明模型在瞎蒙，直接丢弃或返回“无法回答”。

#### 3. 知识图谱增强 (Knowledge Graph RAG)
向量搜索（Vector Search）只能理解语义相似度，很难理解复杂的实体关系。
结合知识图谱（GraphRAG），可以将实体之间的关系硬编码。当模型试图编造“埃隆·马斯克是苹果的CEO”时，知识图谱会直接校验出 `(Elon Musk) --[CEO of]--> (Tesla/SpaceX)`，从而打断幻觉。

---

### 第四层：模型微调层（Fine-tuning）—— “大脑改造”

如果上述方法都不够，你需要动“手术”。

#### 1. SFT (监督微调) 训练“拒答能力”
构建一个数据集，其中包含大量**无法回答**的问题，并将标准答案设为“根据现有信息无法回答”。
通过微调，强行修改模型的权重，让它学会**谦虚**，知道自己的边界在哪里。

#### 2. DPO / RLHF (偏好对齐)
训练一个奖励模型（Reward Model），当模型产生幻觉时给予惩罚，当模型诚实地说“不知道”或引用正确时给予奖励。

---

### 总结：如何构建一个低幻觉系统？

如果你在设计一个企业级应用，建议的实施路径是：

1.  **基础防线：** 必须上 RAG，且必须用 **Hybrid Search** 保证召回质量。
2.  **Prompt 约束：** System Prompt 必须包含“不知则不知”的指令，并要求引用来源。
3.  **后处理：** 检查模型生成的引用链接是否真实存在（防止编造假链接）。
4.  **进阶防线（如果成本允许）：** 加入 CoVE（核验链）或 Self-Correction（自我修正）步骤，虽然会增加 Token 消耗和延迟，但能大幅提升准确率。

### 想要我为你设计一个针对特定场景（比如法律、医疗或客服）的防幻觉 Prompt 模板吗？
