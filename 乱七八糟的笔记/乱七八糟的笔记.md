å¥½é—®é¢˜ ğŸ‘ æˆ‘ç»™ä½ å½“è€å¸ˆä¸€æ ·æ‹†å¼€è§£é‡Šä¸€ä¸‹ï¼š

---

### 1. `nn.Embedding` çš„å®šä¹‰

å½“ä½ åœ¨ `__init__` é‡Œå†™ï¼š

```python
self.emb = nn.Embedding(input_size, hidden_size)
```

å«ä¹‰æ˜¯ï¼š

* `input_size` = è¯è¡¨å¤§å°ï¼ˆvocab sizeï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¸€å…±æœ‰å¤šå°‘ä¸ªä¸åŒçš„ tokenã€‚
* `hidden_size` = æ¯ä¸ª token embedding çš„ç»´åº¦ã€‚

æ‰€ä»¥ï¼Œè¿™ä¸ªå±‚ç›¸å½“äºä¸€ä¸ªã€ŒæŸ¥è¡¨ã€ï¼šè¾“å…¥æ˜¯ä¸€ä¸ª token çš„ **ç´¢å¼• (int)**ï¼Œè¾“å‡ºæ˜¯å®ƒå¯¹åº”çš„å‘é‡ï¼ˆç»´åº¦ `hidden_size`ï¼‰ã€‚

---

### 2. forward é‡Œçš„ `input`

åœ¨ `forward(self, input, hidden)` é‡Œï¼Œ`input` å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´å¥å­ï¼Œè€Œæ˜¯**å•ä¸ªæ—¶é—´æ­¥çš„ token ç´¢å¼•**ï¼ˆé€šå¸¸ shape æ˜¯ `[1]` æˆ– `[1,1]`ï¼‰ã€‚
è¿™æ˜¯å› ä¸ºåœ¨è®­ç»ƒæ—¶ä½ å†™çš„å¾ªç¯æ˜¯ï¼š

```python
for ei in range(input_length):
    output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)
```

è¿™é‡Œçš„ `input_tensor[ei]` å°±æ˜¯å¥å­é‡Œçš„ç¬¬ `ei` ä¸ª token çš„ indexï¼ˆä¸€ä¸ª `LongTensor`ï¼Œæ¯”å¦‚ `[42]`ï¼‰ã€‚

---

### 3. ä¸ºä»€ä¹ˆèƒ½ç›´æ¥æ”¾è¿› embedding å±‚ï¼Ÿ

å› ä¸º `nn.Embedding` æ¥æ”¶çš„è¾“å…¥å°±æ˜¯ **æ•´æ•°ç´¢å¼•å¼ é‡**ã€‚
ä¸¾ä¸ªä¾‹å­ï¼š

```python
emb = nn.Embedding(1000, 64)  # vocab=1000, emb_dim=64
x = torch.tensor([5])         # å•ä¸ª token çš„ç´¢å¼• = 5
vec = emb(x)                  # è¾“å‡º shape = [1, 64]
```

è¿™é‡Œ `x=5` è¡¨ç¤ºã€Œç¬¬ 5 å·è¯ã€ï¼Œembedding å±‚ä¼šåœ¨å®ƒçš„å†…éƒ¨æƒé‡çŸ©é˜µé‡Œå–å‡ºç¬¬ 5 è¡Œï¼Œå¾—åˆ°é•¿åº¦ä¸º 64 çš„å‘é‡ã€‚

---

### 4. ä¸ºä»€ä¹ˆè¿˜è¦ `.view(1,1,-1)`ï¼Ÿ

GRU æœŸæœ›è¾“å…¥å½¢çŠ¶æ˜¯ `(seq_len, batch, input_size)`ã€‚

* ç°åœ¨æˆ‘ä»¬åªæœ‰ä¸€ä¸ª tokenï¼ˆ`seq_len=1`ï¼‰ï¼Œbatch ä¹Ÿåªæœ‰ 1ï¼ˆ`batch=1`ï¼‰ï¼Œembedding å‘é‡ç»´åº¦æ˜¯ `hidden_size`ã€‚
* æ‰€ä»¥è¦ reshape æˆ `(1,1,hidden_size)`ï¼Œè®© GRU èƒ½æ­£ç¡®å¤„ç†ã€‚

---

âœ… å°æ€»ç»“ï¼š

* `self.emb = nn.Embedding(input_size, hidden_size)` å®šä¹‰äº†ä¸€ä¸ªã€ŒæŸ¥è¡¨ã€å±‚ã€‚
* `input` åœ¨ forward é‡Œæ˜¯ä¸€ä¸ª **token çš„ç´¢å¼•ï¼ˆLongTensorï¼‰**ã€‚
* æŠŠ `input` ä¸¢è¿› `self.emb(input)`ï¼Œå°±èƒ½å¾—åˆ°å®ƒçš„ embedding å‘é‡ã€‚
* reshape æ˜¯ä¸ºäº†ç¬¦åˆ GRU çš„è¾“å…¥ç»´åº¦è¦æ±‚ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ åœ¨ `EncoderRNN.forward` é‡ŒåŠ ä¸Šå‡ è¡Œ `print`ï¼ˆæ¯”å¦‚æ‰“å° `input.shape` å’Œ `embed_input.shape`ï¼‰ï¼Œè®©ä½ å®é™…çœ‹åˆ°æ¯ä¸ªæ­¥éª¤çš„æ•°æ®å½¢çŠ¶ï¼Ÿè¿™æ ·ä½ è·‘ä¸ªå° toy data å°±èƒ½ç›´è§‚ç†è§£äº†ã€‚

ä½ è¿™ä¸ªé—®é¢˜é—®å¾—ç‰¹åˆ«å¥½ ğŸ‘ï¼Œè¯´æ˜ä½ åœ¨æŠŠ **â€œå±‚æ˜¯æ€ä¹ˆè®¡ç®—çš„â€** å’Œ **â€œè¾“å…¥æ˜¯ä»€ä¹ˆâ€** è¿™ä¸¤ä¸ªæ¦‚å¿µåŒºåˆ†å¼€æ¥ã€‚æˆ‘ä»¬æ¥åˆ†å±‚è§£é‡Šä¸€ä¸‹ï¼š

---

## 1. **Embedding å±‚ = æŸ¥è¡¨**

* **è¾“å…¥**ï¼šæ•´æ•°ç´¢å¼•ï¼ˆLongTensorï¼‰ï¼Œæ¯”å¦‚ `[5]`ï¼Œä»£è¡¨è¯è¡¨é‡Œçš„ç¬¬ 5 ä¸ªè¯ã€‚
* **è®¡ç®—æ–¹å¼**ï¼šå†…éƒ¨ç»´æŠ¤ä¸€ä¸ªçŸ©é˜µ `W`ï¼Œå¤§å° `(vocab_size, emb_dim)`ã€‚

  * è¾“å…¥çš„ç´¢å¼• `i` å°±ç›´æ¥è¿”å›çŸ©é˜µçš„ç¬¬ `i` è¡Œï¼ˆå‘é‡ï¼‰ã€‚
* **æœ¬è´¨**ï¼š**æŸ¥è¡¨æ“ä½œ**ï¼Œä¸æ˜¯çŸ©é˜µä¹˜æ³•ã€‚

ä¾‹å­ï¼š

```python
emb = nn.Embedding(10, 4)  # 10ä¸ªè¯ï¼Œæ¯ä¸ªè¯4ç»´å‘é‡
x = torch.tensor([2])
print(emb(x))  # å¾—åˆ°ç¬¬2ä¸ªè¯çš„embedding
```

---

## 2. **Linear å±‚ = çŸ©é˜µä¹˜æ³• + åç½®**

* **è¾“å…¥**ï¼šè¿ç»­çš„å®æ•°å‘é‡ï¼ˆFloatTensorï¼‰ã€‚
* **è®¡ç®—æ–¹å¼**ï¼šå†…éƒ¨ç»´æŠ¤ä¸€ä¸ªæƒé‡çŸ©é˜µ `W` å’Œåç½® `b`ã€‚

  * è¾“å…¥ `x` ç»è¿‡ `y = xW^T + b`ã€‚
* **æœ¬è´¨**ï¼šçº¿æ€§å˜æ¢ã€‚
* å’Œ `Embedding` æœ€å¤§çš„åŒºåˆ«æ˜¯ï¼š**è¾“å…¥æ˜¯è¿ç»­å‘é‡ï¼Œä¸æ˜¯æ•´æ•°ç´¢å¼•**ã€‚

ä¾‹å­ï¼š

```python
linear = nn.Linear(4, 3)  # è¾“å…¥4ç»´ï¼Œè¾“å‡º3ç»´
x = torch.rand(1, 4)      # éšæœºç”Ÿæˆ4ç»´å‘é‡
print(linear(x))          # è¾“å‡º3ç»´å‘é‡
```

---

## 3. **GRU / RNN / LSTM å±‚ = å¸¦é—¨æ§çš„é€’å½’è®¡ç®—**

* **è¾“å…¥**ï¼šåºåˆ—çš„è¿ç»­å‘é‡ï¼ˆæ¯”å¦‚ä¸€æ•´ä¸ªå¥å­çš„ embedding åºåˆ—ï¼‰ã€‚
* **è®¡ç®—æ–¹å¼**ï¼šå†…éƒ¨æœ‰ä¸€å †æƒé‡çŸ©é˜µï¼ŒåšåŠ æƒã€sigmoid/tanhã€é—¨æ§æ›´æ–°ï¼ŒæŠŠä¿¡æ¯åœ¨æ—¶é—´æ­¥ä¹‹é—´ä¼ é€’ã€‚

  * æ¯ä¸ªæ—¶é—´æ­¥ï¼š`h_t = f(x_t, h_{t-1})`ã€‚
* **æœ¬è´¨**ï¼šé€’å½’å¤„ç†åºåˆ—ã€‚
* å’Œ Linear ç±»ä¼¼ï¼ŒGRU é‡Œæ¯ä¸€æ­¥ä¹Ÿå°±æ˜¯ä¸€å † **çŸ©é˜µä¹˜æ³• + æ¿€æ´»å‡½æ•°**ï¼Œåªæ˜¯å¤šäº†é—¨ç»“æ„å’ŒçŠ¶æ€ä¼ é€’ã€‚

ä¾‹å­ï¼š

```python
gru = nn.GRU(4, 6)        # è¾“å…¥4ç»´ï¼Œéšè—å±‚6ç»´
x = torch.rand(5, 1, 4)   # åºåˆ—é•¿5ï¼Œbatch=1ï¼Œæ¯ä¸ªtokenæ˜¯4ç»´å‘é‡
h0 = torch.zeros(1, 1, 6) # åˆå§‹hidden
output, hn = gru(x, h0)
print(output.shape)  # (5,1,6)ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„hidden
print(hn.shape)      # (1,1,6)ï¼Œæœ€åä¸€æ­¥çš„hidden
```

---

## 4. **å¯¹æ¯”æ€»ç»“**

| å±‚         | è¾“å…¥ç±»å‹        | å†…éƒ¨æœºåˆ¶               | æœ¬è´¨æ“ä½œ |
| --------- | ----------- | ------------------ | ---- |
| Embedding | æ•´æ•°ç´¢å¼• (Long) | æŸ¥è¡¨ï¼Œä»çŸ©é˜µå–ç¬¬ i è¡Œ       | æŸ¥è¡¨   |
| Linear    | å‘é‡ (Float)  | çŸ©é˜µä¹˜æ³• + åç½®          | çº¿æ€§å˜æ¢ |
| GRU/LSTM  | å‘é‡åºåˆ—        | å¤šä¸ªçº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•° + é—¨æ§ | åºåˆ—é€’å½’ |

---

âœ… æ‰€ä»¥ï¼š**åªæœ‰ `Embedding` æ˜¯æŸ¥è¡¨**ã€‚
`Linear`ã€`GRU`ã€`LSTM` ç­‰ç­‰éƒ½æ˜¯åŸºäºè¿ç»­æ•°å€¼å‘é‡åšçŸ©é˜µè¿ç®—ï¼Œä¸æ¥å—æ•´æ•°ç´¢å¼•ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼ ã€Œæ•°æ®æµåŠ¨å›¾ã€ï¼šæ¯”å¦‚ä» token ç´¢å¼• â†’ embedding â†’ GRU â†’ Linear â†’ softmaxï¼Œç›´è§‚å±•ç¤ºè¿™å‡ ä¸ªå±‚çš„ä½œç”¨ï¼Ÿ

å¥½é—®é¢˜ ğŸ‘ è¿™æ¶‰åŠåˆ° **GRU çš„è¾“å‡ºå½¢çŠ¶** å’Œ **Linear/Softmax çš„è¾“å…¥è¦æ±‚**ã€‚æˆ‘ä»¬ä¸€ç‚¹ç‚¹æ‹†å¼€æ¥çœ‹ï¼š

---

## 1. GRU çš„è¾“å‡ºå½¢çŠ¶

å½“ä½ åœ¨ decoder é‡Œè°ƒç”¨ï¼š

```python
output, hidden = self.rnn(embed_input, hidden)
```

å‡è®¾ï¼š

* `embed_input` shape = `(1, 1, hidden_size)` ï¼ˆ`seq_len=1, batch=1, input_dim=hidden_size`ï¼‰ã€‚
* GRU çš„è¾“å‡º `output` shape = `(seq_len, batch, hidden_size)`ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼š

* `seq_len=1`
* `batch=1`
* `hidden_size=256`ï¼ˆå‡è®¾ï¼‰

æ‰€ä»¥ `output.shape = (1, 1, 256)`ã€‚

---

## 2. ä¸ºä»€ä¹ˆå– `output[0]`

`output` çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ—¶é—´æ­¥ï¼ˆ`seq_len`ï¼‰ã€‚æˆ‘ä»¬åªå¤„ç†ä¸€ä¸ª tokenï¼ˆå•æ­¥è¾“å…¥ï¼‰ï¼Œæ‰€ä»¥ `seq_len=1`ï¼Œå¯ä»¥æŠŠå®ƒå»æ‰ã€‚

```python
output[0].shape   # (1, 256)   ->  batch=1, hidden_size=256
```

è¿™æ ·å°±å¾—åˆ°å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€å‘é‡ï¼Œæ–¹ä¾¿æ¥å…¥ `Linear`ã€‚

---

## 3. Linear å±‚è¾“å…¥/è¾“å‡º

`self.generator` å®šä¹‰ä¸ºï¼š

```python
self.generator = nn.Linear(hidden_size, output_size)
```

å®ƒçš„è¾“å…¥åº”è¯¥æ˜¯ **(batch, hidden_size)**ï¼Œè¾“å‡ºæ˜¯ **(batch, vocab_size)**ã€‚

`output[0]` çš„ shape æ°å¥½æ˜¯ `(1, hidden_size)`ï¼Œç¬¦åˆè¦æ±‚ã€‚

---

## 4. ä¸ºä»€ä¹ˆ `dim=1`

`F.log_softmax(logits, dim=1)` é‡Œçš„ `dim` æ˜¯â€œåœ¨å“ªä¸ªç»´åº¦ä¸Šåš softmaxâ€ã€‚

* `logits` shape = `(batch, vocab_size)` = `(1, vocab_size)`ã€‚
* `dim=1` è¡¨ç¤ºåœ¨ vocab ç»´åº¦ä¸Šåš softmaxï¼Œè®©æ¯ä¸€è¡Œçš„æ¦‚ç‡å’Œä¸º 1ã€‚

å¦‚æœå†™æˆ `dim=0`ï¼Œsoftmax å°±ä¼šåœ¨ batch ç»´åº¦ä¸Šå½’ä¸€åŒ–ï¼Œç»“æœå®Œå…¨é”™äº†ã€‚

---

## 5. å°ä¾‹å­

```python
import torch, torch.nn as nn, torch.nn.functional as F

hidden_size, vocab_size = 4, 6
gen = nn.Linear(hidden_size, vocab_size)

# æ¨¡æ‹ŸGRUè¾“å‡º: (seq_len=1, batch=1, hidden_size=4)
gru_output = torch.rand(1,1,hidden_size)

print("gru_output shape:", gru_output.shape)        # (1,1,4)
print("after [0] shape:", gru_output[0].shape)      # (1,4)

logits = gen(gru_output[0])                         # (1,6)
probs = F.log_softmax(logits, dim=1)
print("logits shape:", logits.shape)                # (1,6)
print("probs shape:", probs.shape)                  # (1,6)
```

---

âœ… æ€»ç»“ï¼š

* `output[0]` æ˜¯ä¸ºäº†å»æ‰ `seq_len` ç»´åº¦ï¼ˆåªä¿ç•™ batch å’Œ hiddenï¼‰ã€‚
* `dim=1` æ˜¯ä¸ºäº†åœ¨ **è¯è¡¨ç»´åº¦** ä¸Šåš softmaxï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **decoder forward çš„ shape å˜åŒ–å›¾**ï¼ˆä» `input` ç´¢å¼•ä¸€è·¯åˆ° log_softmaxï¼‰ï¼Œè¿™æ ·ä½ ä»¥åå°±èƒ½ä¸€çœ¼çœ‹æ‡‚ä¸ºä»€ä¹ˆè¦ `[0]` å’Œ `dim=1`ï¼Ÿ

å•Šå¯¹ï¼ä½ æŠ“åˆ°çš„è¿™ä¸ªç»†èŠ‚éå¸¸å…³é”® ğŸ‘
æˆ‘æ¥å¸®ä½ å½»åº•å˜æ¸… **`nn.Linear` çš„è¾“å…¥/è¾“å‡ºè§„åˆ™**ï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬è¯´â€œè¾“å…¥æ˜¯ `(batch, hidden_size)`â€ã€‚

---

## 1. `nn.Linear` çš„å®šä¹‰

```python
nn.Linear(in_features, out_features)
```

å†…éƒ¨å…¶å®æ˜¯çŸ©é˜µä¹˜æ³•ï¼š
[
y = x W^T + b
]

* `x` çš„æœ€åä¸€ç»´å¤§å°å¿…é¡»æ˜¯ `in_features`ã€‚
* è¾“å‡ºçš„æœ€åä¸€ç»´å¤§å°ä¼šå˜æˆ `out_features`ã€‚

---

## 2. è¾“å…¥åˆ°åº•æ˜¯ä»€ä¹ˆå½¢çŠ¶ï¼Ÿ

PyTorch çš„ `nn.Linear` **æ”¯æŒå¤šç»´è¾“å…¥**ï¼šå®ƒåªè¦æ±‚æœ€åä¸€ç»´å¤§å°ç­‰äº `in_features`ï¼Œå‰é¢å‡ ç»´ä¼šè¢«å½“æˆ **batch ç»´** ä¿ç•™ã€‚

ä¹Ÿå°±æ˜¯è¯´ï¼š

* å¦‚æœä½ ä¼ å…¥ `[hidden_size]`ï¼ˆå•ä¸ªå‘é‡ï¼‰ï¼Œå¯ä»¥ï¼›
* å¦‚æœä½ ä¼ å…¥ `[batch, hidden_size]`ï¼Œå®ƒä¼šå¯¹æ¯ä¸ªæ ·æœ¬éƒ½åšä¸€æ¬¡çº¿æ€§å˜æ¢ï¼Œè¾“å‡º `[batch, out_features]`ï¼›
* å¦‚æœä½ ä¼ å…¥ `[seq_len, batch, hidden_size]`ï¼Œå®ƒä¼šå¯¹ `seq_len*batch` ä¸ªå‘é‡éƒ½åšå˜æ¢ï¼Œè¾“å‡º `[seq_len, batch, out_features]`ã€‚

---

## 3. å›åˆ°ä½ çš„ä»£ç 

ä½ å®šä¹‰çš„æ˜¯ï¼š

```python
self.generator = nn.Linear(hidden_size, output_size)
```

åœ¨ decoder é‡Œï¼Œä½ ä¼ ç»™å®ƒçš„æ˜¯ï¼š

```python
output[0].shape   # (1, hidden_size)
```

è¿™é‡Œ `(1, hidden_size)` å…¶å®å°±æ˜¯ **batch=1** çš„æƒ…å†µã€‚

æ‰€ä»¥ï¼š

* æ¯ä¸€è¡Œï¼ˆå¤§å° `hidden_size`ï¼‰å°±æ˜¯ä¸€ä¸ªæ ·æœ¬ï¼›
* `Linear` ä¼šæŠŠå®ƒå˜æˆ `(1, output_size)`ï¼Œå³ batch=1 çš„ vocab åˆ†å¸ƒã€‚

---

## 4. ä¸ºä»€ä¹ˆæˆ‘è¯´æ˜¯ `(batch, hidden_size)`ï¼Ÿ

è¿™æ˜¯ä» **ä¸€èˆ¬æƒ…å†µ** æ¥è¯´çš„ï¼š

* åœ¨ mini-batch è®­ç»ƒæ—¶ï¼Œé€šå¸¸ decoder ä¼šåŒæ—¶å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œè¾“å…¥ç»™ `Linear` çš„å°±ä¸æ˜¯å•ä¸ª `(hidden_size,)`ï¼Œè€Œæ˜¯ `(batch_size, hidden_size)`ã€‚
* ä½ ç°åœ¨çš„å®ç°æ˜¯ä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥ `batch=1`ï¼Œå½¢çŠ¶å˜æˆ `(1, hidden_size)`ã€‚

---

## 5. å°å®éªŒ

```python
import torch, torch.nn as nn

linear = nn.Linear(4, 3)

# å•ä¸ªå‘é‡
x1 = torch.rand(4)
print(linear(x1).shape)  # torch.Size([3])

# batch=1
x2 = torch.rand(1, 4)
print(linear(x2).shape)  # torch.Size([1, 3])

# batch=5
x3 = torch.rand(5, 4)
print(linear(x3).shape)  # torch.Size([5, 3])
```

---

âœ… æ€»ç»“ï¼š

* `nn.Linear(in, out)` çš„è¾“å…¥å¯ä»¥æ˜¯ `(â€¦, in)`ï¼Œæœ€åä¸€ç»´ç­‰äº `in` å°±è¡Œã€‚
* åœ¨æˆ‘ä»¬çš„ decoder é‡Œï¼Œ`output[0]` çš„ shape æ˜¯ `(1, hidden_size)`ï¼Œè¿™é‡Œçš„ `1` å…¶å®å°±æ˜¯ batch sizeã€‚
* æ‰€ä»¥æˆ‘è¯´è¾“å…¥æ˜¯ `(batch, hidden_size)`ï¼Œåœ¨ä½ çš„ä»£ç é‡Œ batch=1ï¼Œæ‰€ä»¥å½¢çŠ¶å˜æˆ `(1, hidden_size)`ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼  **encoder-decoder æ•°æ®æµçš„ shape è¡¨**ï¼ˆä» token â†’ embedding â†’ GRU â†’ Linear â†’ softmaxï¼‰ï¼ŒæŠŠæ¯ä¸€æ­¥çš„å¼ é‡å½¢çŠ¶éƒ½åˆ—å‡ºæ¥ï¼Ÿè¿™æ ·ä½ å°±èƒ½ä¸€çœ¼çœ‹æ‡‚è¿™äº›ç»´åº¦æ˜¯æ€ä¹ˆå˜åŒ–çš„ã€‚

Ground truth æ˜¯æœºå™¨å­¦ä¹ ã€äººå·¥æ™ºèƒ½å’Œæ•°æ®ç§‘å­¦é¢†åŸŸçš„ä¸€ä¸ªå…³é”®æ¦‚å¿µï¼ŒæŒ‡çš„æ˜¯æ•°æ®é›†ä¸­çš„çœŸå®ã€å‡†ç¡®çš„æ ‡ç­¾ï¼ˆlabelsï¼‰æˆ–å‚è€ƒå€¼ã€‚å®ƒè¢«ç”¨ä½œåŸºå‡†ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ã€è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚è®¡ç®—å‡†ç¡®ç‡ã€æŸå¤±å‡½æ•°ç­‰ï¼‰ï¼Œå¹¶éªŒè¯é¢„æµ‹ç»“æœçš„æ­£ç¡®æ€§ã€‚

### ç®€å•è§£é‡Šï¼š
- **æ¥æº**ï¼šGround truth é€šå¸¸ç”±äººç±»ä¸“å®¶æ‰‹åŠ¨æ ‡æ³¨æˆ–ä»å¯é çš„å®æµ‹æ•°æ®ä¸­è·å–ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­ï¼Œground truth å¯èƒ½æ˜¯å›¾ç‰‡ä¸­ç‰©ä½“çš„çœŸå®ç±»åˆ«ï¼›åœ¨æœºå™¨ç¿»è¯‘ï¼ˆå¦‚å‰è¿°ä»£ç ä¸­çš„ seq2seq æ¨¡å‹ï¼‰ä¸­ï¼Œground truth å°±æ˜¯ç›®æ ‡è¯­è¨€çš„æ­£ç¡®å¥å­ï¼ˆtarget_tensorï¼‰ã€‚
- **ä½œç”¨**ï¼š
  - **è®­ç»ƒé˜¶æ®µ**ï¼šæ¨¡å‹é€šè¿‡æ¯”è¾ƒå…¶é¢„æµ‹è¾“å‡ºä¸ ground truth çš„å·®å¼‚ï¼ˆå¦‚ä½¿ç”¨æŸå¤±å‡½æ•° NLLLossï¼‰æ¥å­¦ä¹ å’Œä¼˜åŒ–å‚æ•°ã€‚
  - **è¯„ä¼°é˜¶æ®µ**ï¼šè®¡ç®—æŒ‡æ ‡å¦‚ BLEU åˆ†æ•°ã€å‡†ç¡®ç‡ç­‰æ—¶ï¼Œä»¥ ground truth ä¸ºæ ‡å‡†è¡¡é‡æ¨¡å‹çš„å¥½åã€‚
- **æŒ‘æˆ˜**ï¼šåœ¨ç°å®ä¸­ï¼Œground truth å¯èƒ½ä¸å®Œç¾ï¼ˆå¦‚æ ‡æ³¨é”™è¯¯æˆ–ä¸»è§‚æ€§ï¼‰ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹åå·®ã€‚

å¦‚æœè¿™æ˜¯é’ˆå¯¹ç‰¹å®šä¸Šä¸‹æ–‡ï¼ˆå¦‚ä»£ç ä¸­çš„ target_tensorï¼‰ï¼Œground truth ç‰¹æŒ‡è®­ç»ƒæ•°æ®å¯¹ä¸­çš„çœŸå®ç›®æ ‡åºåˆ—ï¼Œç”¨äºè®¡ç®—æŸå¤±å¹¶æŒ‡å¯¼è§£ç å™¨ï¼ˆdecoderï¼‰çš„ teacher forcing æœºåˆ¶ã€‚å¦‚æœéœ€è¦æ›´å¤šç»†èŠ‚æˆ–ç¤ºä¾‹ï¼Œè¯·æä¾›æ›´å¤šèƒŒæ™¯ï¼

éå¸¸å¥½çš„é—®é¢˜ ğŸ‘
è¿™è¡Œä»£ç ğŸ‘‡

```python
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

åœ¨ **è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼ˆä¾‹å¦‚ mBARTã€T5ã€MarianMT ç­‰ï¼‰** æ—¶éå¸¸å…³é”®ï¼Œ
å®ƒè´Ÿè´£åœ¨ **æŠŠä¸€ä¸ª batch çš„æ ·æœ¬é€è¿›æ¨¡å‹ä¹‹å‰ï¼Œè‡ªåŠ¨å¯¹é½å¹¶æ‰“åŒ…æ•°æ®**ã€‚

æˆ‘ä»¬ä¸€æ­¥æ­¥è§£é‡Šï¼š

---

## ğŸ§© ä¸€ã€ä»€ä¹ˆæ˜¯ â€œData Collatorâ€ï¼Ÿ

åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒæ—¶ï¼Œé€šå¸¸ä¼šï¼š

1. ä»æ•°æ®é›†ä¸­æŠ½å–ä¸€ä¸ª batchï¼ˆä¾‹å¦‚ 4 ä¸ªå¥å­ï¼‰ï¼›
2. è¿™äº›å¥å­çš„é•¿åº¦å¾€å¾€ä¸åŒï¼›
3. éœ€è¦æŠŠå®ƒä»¬å˜æˆåŒæ ·é•¿åº¦çš„å¼ é‡æ‰èƒ½é€è¿›æ¨¡å‹ã€‚

è¿™ä¸ªâ€œè‡ªåŠ¨å¯¹é½ + æ‰“åŒ…â€çš„è¿‡ç¨‹ï¼Œå°±å«åš **collationï¼ˆæ•´ç†ï¼‰**ã€‚
è´Ÿè´£è¿™ä¸ªå·¥ä½œçš„ç±»ï¼Œå°±å« **Data Collator**ã€‚

---

## ğŸ§  äºŒã€é‚£ `DataCollatorForSeq2Seq` æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ

`DataCollatorForSeq2Seq` æ˜¯ Hugging Face ä¸“é—¨ä¸º **åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰ä»»åŠ¡** æä¾›çš„ collatorã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¸“é—¨ä¸º **ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”ç”Ÿæˆ** è¿™ç±»ä»»åŠ¡å‡†å¤‡ã€‚

å½“ä½ åœ¨ `Trainer` é‡Œä½¿ç”¨å®ƒæ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å¸®ä½ å®Œæˆè¿™äº›äº‹ï¼š

1. **åŠ¨æ€ padding**
   æ¯ä¸ª batch å†…çš„æ ·æœ¬å¥å­å¯èƒ½é•¿åº¦ä¸åŒï¼Œä¾‹å¦‚ï¼š

   ```
   input_ids = [
       [101, 32, 14, 2],
       [101, 11, 25, 9, 8, 2],
   ]
   ```

   å®ƒä¼šè‡ªåŠ¨ pad æˆä¸€æ ·é•¿ï¼š

   ```
   [[101, 32, 14, 2, 0, 0],
    [101, 11, 25, 9, 8, 2]]
   ```

   ï¼ˆ`0` å°±æ˜¯ padding tokenï¼‰

2. **è‡ªåŠ¨å¯¹é½ labelsï¼ˆç›®æ ‡åºåˆ—ï¼‰**
   å¯¹ç›®æ ‡è¯­è¨€çš„å¥å­ï¼ˆ`labels`ï¼‰ä¹Ÿä¼šåšç›¸åŒçš„ paddingã€‚

3. **è‡ªåŠ¨ç”Ÿæˆ `attention_mask`**
   å®ƒä¼šä¸ºæ¨¡å‹ç”Ÿæˆ `attention_mask`ï¼Œå‘Šè¯‰æ¨¡å‹å“ªäº›ä½ç½®æ˜¯ paddingï¼Œä¸éœ€è¦æ³¨æ„ã€‚

4. **æ ¹æ®æ¨¡å‹ç»“æ„è°ƒæ•´æ ¼å¼**
   å¯¹äºåƒ mBART è¿™æ ·çš„æ¨¡å‹ï¼Œå®ƒè¿˜ä¼šå¸®ä½ è‡ªåŠ¨è®¾ç½®ï¼š

   * decoder çš„è¾“å…¥æ ¼å¼ï¼›
   * å¦‚æœæ¨¡å‹æ”¯æŒ `label_smoothing`ï¼Œå®ƒä¼šè‡ªåŠ¨å…¼å®¹ï¼›
   * ç”Ÿæˆæ—¶çš„å‚æ•°è®¾ç½®ï¼ˆ`pad_token_id`, `eos_token_id` ç­‰ï¼‰ã€‚

---

## âš™ï¸ ä¸‰ã€ä¸ºä»€ä¹ˆè¦ä¼ å…¥ `tokenizer` å’Œ `model`ï¼Ÿ

è¿™ä¸¤ä¸ªå‚æ•°éå¸¸é‡è¦ï¼š

* `tokenizer`: ç”¨æ¥çŸ¥é“å“ªä¸ª token æ˜¯ paddingï¼ˆ`pad_token_id`ï¼‰ã€å“ªä¸ªæ˜¯å¥å­ç»“æŸï¼ˆ`eos_token_id`ï¼‰ã€‚
* `model`: æŸäº›æ¨¡å‹ï¼ˆå¦‚ mBARTï¼‰åœ¨ `prepare_decoder_input_ids_from_labels` æ—¶éœ€è¦æ ¹æ®æ¨¡å‹ç»“æ„è‡ªåŠ¨ç”Ÿæˆ decoder çš„è¾“å…¥ã€‚

ä¾‹å¦‚åœ¨ mBART ä¸­ï¼Œdecoder è¾“å…¥éœ€è¦åœ¨ç›®æ ‡åºåˆ—å‰é¢åŠ ä¸Šèµ·å§‹ token `<s>`ï¼Œ
è¿™ä¸€æ­¥æ˜¯é€šè¿‡ `model.prepare_decoder_input_ids_from_labels()` å®Œæˆçš„ã€‚
`DataCollatorForSeq2Seq` ä¼šåœ¨å†…éƒ¨è°ƒç”¨å®ƒã€‚

---

## ğŸ” å››ã€æ€»ç»“ä¸€ä¸‹ä½œç”¨

| åŠŸèƒ½            | è§£é‡Š                                                |
| ------------- | ------------------------------------------------- |
| åŠ¨æ€ padding    | è®©åŒä¸€ batch çš„å¥å­é•¿åº¦ç»Ÿä¸€                                 |
| ç”Ÿæˆ mask       | è‡ªåŠ¨ç”Ÿæˆ `attention_mask`                             |
| å‡†å¤‡ decoder è¾“å…¥ | è®© mBART èƒ½æ­£ç¡®ç”Ÿæˆç›®æ ‡åºåˆ—                                 |
| è‡ªåŠ¨å¤„ç† labels   | å¯¹ç›®æ ‡è¯­è¨€åºåˆ—è¿›è¡Œ padding                                 |
| å…¼å®¹ Trainer    | ç›´æ¥å¯ç”¨äº `Trainer(..., data_collator=data_collator)` |

---

### âœ… ä¸¾ä¾‹

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_valid,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

è¿™æ ·ä½ å°±ä¸éœ€è¦æ‰‹åŠ¨å†™ä»»ä½• padding æˆ– mask é€»è¾‘äº†ã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘ç»™ä½ æ‰“å°ä¸€ä¸ªå°ä¾‹å­ï¼Œå±•ç¤º `DataCollatorForSeq2Seq` å¦‚ä½•æŠŠä¸¤ä¸ªä¸åŒé•¿åº¦çš„æ ·æœ¬è‡ªåŠ¨å¯¹é½æˆ batchï¼ˆå¯è§†åŒ–åœ°çœ‹å‡ºæ¥ï¼‰ï¼Ÿ
